[
  {
    "url": "https://github.com/LUOyk1999/dropout-theory",
    "paper_url": "https://openreview.net/pdf?id=PwxYoMvmvy",
    "paper_id": "PwxYoMvmvy",
    "contexts": [
      "To validate our theoretical analysis, we conducted extensive experiments on a variety of datasets,considering both node-level and graph-level tasks. We implemented dropout technique on severalpopular GNN architectures: GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017),GAT (Veliˇckovi´c et al., 2018), and GatedGCN (Bresson & Laurent, 2017). For each model, wecompared the performance with and without dropout. Our code is available at https://github.com/LUOyk1999/dropout-theory"
    ]
  },
  {
    "url": "https://openreview.net/forum?id=xkljKdGe4E",
    "paper_url": "https://openreview.net/pdf?id=PwxYoMvmvy",
    "paper_id": "PwxYoMvmvy",
    "contexts": [
      "Yuankai Luo, Lei Shi, and Xiao-Ming Wu. Classic GNNs are strong baselines: Reassessing GNNs\nfor node classification. In The Thirty-eight Conference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=\nxkljKdGe4E.\nYuankai Luo, Lei Shi, and Xiao-Ming Wu. Unlocking the potential of classic gnns for graph-level"
    ]
  },
  {
    "url": "https://openreview.net/forum?id=",
    "paper_url": "https://openreview.net/pdf?id=PwxYoMvmvy",
    "paper_id": "PwxYoMvmvy",
    "contexts": [
      "Yuankai Luo, Lei Shi, and Xiao-Ming Wu. Classic GNNs are strong baselines: Reassessing GNNs\nfor node classification. In The Thirty-eight Conference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=\nxkljKdGe4E.\nYuankai Luo, Lei Shi, and Xiao-Ming Wu. Unlocking the potential of classic gnns for graph-level"
    ]
  },
  {
    "url": "13.w/o",
    "paper_url": "https://openreview.net/pdf?id=ONfWFluZBI",
    "paper_id": "ONfWFluZBI",
    "contexts": [
      "matching trial from the training set in Figure 12 and b) a distribution of the distances (in terms of R2\nbetween the data from the test and train trials) between all test trials of one of a random test set and\ntheir closest trial from the training dataset in Figure 13.\nw/o dynamics\nours"
    ]
  },
  {
    "url": "https://github.com/dynamical-inference/patchsae",
    "paper_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "paper_id": "imT03YXlG2",
    "contexts": [
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAIImageNet templates for zero-shot classification (link). Code, model weights and raw results areavailable at https://github.com/dynamical-inference/patchsae. We only usedpublicly available datasets following the official implementation of MaPLe (dataset descriptions)which are cited in the main text.",
      "Figure 10: SAE latents are generalizable to different datasets. Reference images of two SAE latents (a)(top) and (b) (bottom) from five datasets. We present label and class name above each image. The latentstatistics log10 of activated frequency, log10 of mean activation, and label entropy values computed from eachdataset are summarized as (f, a, e) below four reference images. More examples are shown in the interactivedemo.",
      "Figure 12: interactive demo. (a) Select input image. Specifying patch is also available. (b) Select imageencoder backbone for SAE latents. We provide CLIP as default and MaPLe trained on different datasets forcomparisons. We show image-level and patch-level (if a patch is specified) SAE latent activations. (c) Top SAElatents (commonly / only in CLIP / only in MaPLe) are selectable. We show (d) segmentation mask and (e)reference images (and activation value of each image) for the selected index. We provide (c-1) on/off optionfor segmentation mask in reference images. (c-2) shows reference images with segmentation mask.",
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAI\nImageNet templates for zero-shot classification (link). Code, model weights and raw results are\navailable at https://github.com/dynamical-inference/patchsae. We only used\npublicly available datasets following the official implementation of MaPLe (dataset descriptions)\nwhich are cited in the main text."
    ]
  },
  {
    "url": "https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
    "paper_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "paper_id": "imT03YXlG2",
    "contexts": [
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAIImageNet templates for zero-shot classification (link). Code, model weights and raw results areavailable at https://github.com/dynamical-inference/patchsae. We only usedpublicly available datasets following the official implementation of MaPLe (dataset descriptions)which are cited in the main text."
    ]
  },
  {
    "url": "https://github.com/muzairkhattak/multimodal-prompt-learning?tab=readme-ov-file",
    "paper_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "paper_id": "imT03YXlG2",
    "contexts": [
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAIImageNet templates for zero-shot classification (link). Code, model weights and raw results areavailable at https://github.com/dynamical-inference/patchsae. We only usedpublicly available datasets following the official implementation of MaPLe (dataset descriptions)which are cited in the main text."
    ]
  },
  {
    "url": "https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb",
    "paper_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "paper_id": "imT03YXlG2",
    "contexts": [
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAIImageNet templates for zero-shot classification (link). Code, model weights and raw results areavailable at https://github.com/dynamical-inference/patchsae. We only usedpublicly available datasets following the official implementation of MaPLe (dataset descriptions)which are cited in the main text."
    ]
  },
  {
    "url": "https://github.com/muzairkhattak/multimodal-prompt-learning/blob/main/docs/DATASETS.md",
    "paper_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "paper_id": "imT03YXlG2",
    "contexts": [
      "We used publicly available model checkpoints for CLIP (link) and MaPLe (link). We used OpenAIImageNet templates for zero-shot classification (link). Code, model weights and raw results areavailable at https://github.com/dynamical-inference/patchsae. We only usedpublicly available datasets following the official implementation of MaPLe (dataset descriptions)which are cited in the main text."
    ]
  },
  {
    "url": "https://github.com/apivich-h/pied",
    "paper_url": "https://openreview.net/pdf?id=w7P92BEsb2",
    "paper_id": "w7P92BEsb2",
    "contexts": [
      "1The code for the project can be found at https://github.com/apivich-h/pied.2Examples of PDEs, specifically those in our experiments, can be found in App. D.3For simplicity we assume β is finite-dimensional. In our experiments, we demonstrate how our method can",
      "results in the main paper, and defer the remaining results to App. I. The code for the project can befound at https://github.com/apivich-h/pied.",
      ".\n1\nThe code for the project can be found at https://github.com/apivich-h/pied.\n2\nExamples of PDEs, specifically those in our experiments, can be found in App. D.",
      "Additional details on the experimental setup are listed in App. H. We present a subset of experimental\nresults in the main paper, and defer the remaining results to App. I. The code for the project can be\nfound at https://github.com/apivich-h/pied.\nFinite-dimensional PDE parameters. We first present two ED problems on IPs where the PDE\nparameters corresponds to multiple scalar terms representing certain physical properties of the system."
    ]
  },
  {
    "url": "https://github.com/apivich-h/pied.2",
    "paper_url": "https://openreview.net/pdf?id=w7P92BEsb2",
    "paper_id": "w7P92BEsb2",
    "contexts": [
      ".\n1\nThe code for the project can be found at https://github.com/apivich-h/pied.\n2\nExamples of PDEs, specifically those in our experiments, can be found in App. D."
    ]
  },
  {
    "url": "https://github.com/apivich-h/pied.Finite-dimensional",
    "paper_url": "https://openreview.net/pdf?id=w7P92BEsb2",
    "paper_id": "w7P92BEsb2",
    "contexts": [
      "Additional details on the experimental setup are listed in App. H. We present a subset of experimental\nresults in the main paper, and defer the remaining results to App. I. The code for the project can be\nfound at https://github.com/apivich-h/pied.\nFinite-dimensional PDE parameters. We first present two ED problems on IPs where the PDE\nparameters corresponds to multiple scalar terms representing certain physical properties of the system."
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:256868474",
    "paper_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "paper_id": "FDimWzmcWn",
    "contexts": [
      "URL https://api.semanticscholar.org/CorpusID:",
      "256868474",
      ".",
      "Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Trans. Mach.\nLearn. Res., 2023, 2023. URL https://api.semanticscholar.org/CorpusID:\n256868474.\nShuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang,"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:261556862",
    "paper_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "paper_id": "FDimWzmcWn",
    "contexts": [
      "URL https:",
      "//api.semanticscholar.org/CorpusID:261556862",
      ".",
      "preprint arXiv:2010.03768, 2020.\nTheodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive\narchitectures for language agents. Trans. Mach. Learn. Res., 2024, 2023. URL https:\n//api.semanticscholar.org/CorpusID:261556862.\nMauro Vallati, Lukas Chrpa, Marek Grześ, Thomas Leo McCluskey, Mark Roberts, Scott Sanner,"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:",
    "paper_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "paper_id": "FDimWzmcWn",
    "contexts": [
      "Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Trans. Mach.\nLearn. Res., 2023, 2023. URL https://api.semanticscholar.org/CorpusID:\n256868474.\nShuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang,"
    ]
  },
  {
    "url": "api.semanticscholar.org/CorpusID:261556862.Mauro",
    "paper_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "paper_id": "FDimWzmcWn",
    "contexts": [
      "Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive\narchitectures for language agents. Trans. Mach. Learn. Res., 2024, 2023. URL https:\n//api.semanticscholar.org/CorpusID:261556862.\nMauro Vallati, Lukas Chrpa, Marek Grześ, Thomas Leo McCluskey, Mark Roberts, Scott Sanner,\net al. The 2014 international planning competition: Progress and trends. Ai Magazine, 36(3):"
    ]
  },
  {
    "url": "api.semanticscholar.org/CorpusID:261556862",
    "paper_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "paper_id": "FDimWzmcWn",
    "contexts": [
      "Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive\narchitectures for language agents. Trans. Mach. Learn. Res., 2024, 2023. URL https:\n//api.semanticscholar.org/CorpusID:261556862.\nMauro Vallati, Lukas Chrpa, Marek Grześ, Thomas Leo McCluskey, Mark Roberts, Scott Sanner,\net al. The 2014 international planning competition: Progress and trends. Ai Magazine, 36(3):"
    ]
  }
]