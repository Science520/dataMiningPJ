[
  {
    "url": "http://arxiv.org/abs/2410.09114",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "trophic cyber capabilities benchmark (3cb): Robustly evaluating llm agent cyber offensecapabilities."
    ]
  },
  {
    "url": "http://arxiv.org/abs/2410.09247",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "and Jason Schreiber. 2024. Benchmark inflation: Revealing llm performance gaps usingretro-holdouts."
    ]
  },
  {
    "url": "http://arxiv.org/abs/2411.08813",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "thinking cyberseceval: An llm-aided approach to evaluation critique."
    ]
  },
  {
    "url": "http://arxiv.org/abs/2403.03218",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, AdamKhoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel,Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt,Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, IanSteneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian,Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Ku-maraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M.Esvelt, Alexandr Wang, and Dan Hendrycks. 2024. The wmdp benchmark: Measuringand reducing malicious use with unlearning."
    ]
  },
  {
    "url": "https://aclanthology.org/2024.lrec-main.884",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "Matthias Spielkamp, and Birgit Stark. 2024. Large language models are echo chambers.In Proceedings of the 2024 Joint International Conference on Computational Linguistics,Language Resources and Evaluation (LREC-COLING 2024), pages 10117–10123, Torino,Italia. ELRA and ICCL."
    ]
  },
  {
    "url": "http://arxiv.org/abs/2304.03279",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "side, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023. Do therewards justify the means? measuring trade-offs between rewards and ethical behavior inthe machiavelli benchmark."
    ]
  },
  {
    "url": "http://arxiv.org/abs/2404.03189",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "probabilities also matter: A more faithful metric for faithfulness of free-text explanationsin large language models."
    ]
  },
  {
    "url": "huggingface.co/datasets/anonymous152311",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "pattern.\nand model responses can be found in Figure 3 and Appendix 6. Each pattern is described\nin Section 2.1.\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.",
      "and model responses can be found in Figure 3 and Appendix 6. Each pattern is described\nin Section 2.1.\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.\n2.4 Benchmark construction",
      "you have the Remote: Dev Containers extension and Docker installed.\n3. If you wish not to use Docker, run poetry install\n4. Run dvc pull to pull all the data\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.",
      "3. If you wish not to use Docker, run poetry install\n4. Run dvc pull to pull all the data\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.\nReferences"
    ]
  },
  {
    "url": "huggingface.co/datasets/anonymous152311/darkbench",
    "paper_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "paper_id": "odjMSBSWRt",
    "contexts": [
      "and model responses can be found in Figure 3 and Appendix 6. Each pattern is described\nin Section 2.1.\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.\n2.4 Benchmark construction",
      "3. If you wish not to use Docker, run poetry install\n4. Run dvc pull to pull all the data\nThe DarkBench benchmark is available at huggingface.co/datasets/anonymous152311\n/darkbench.\nReferences"
    ]
  },
  {
    "url": "https://github.com/THU-KEG/RM-Bench",
    "paper_url": "https://openreview.net/pdf?id=QEHrmQPBdd",
    "paper_id": "QEHrmQPBdd",
    "contexts": [
      "Reward models are critical in techniques like Reinforcement Learning from Hu-man Feedback (RLHF) and Inference Scaling Laws, where they guide languagemodel alignment and select optimal responses. Despite their importance, existingreward model benchmarks often evaluate models by asking them to distinguish be-tween responses generated by models of varying power. However, this approachfails to assess reward models on subtle but critical content changes and variationsin style, resulting in a low correlation with policy model performance. To thisend, we introduce RM-BENCH, a novel benchmark designed to evaluate rewardmodels based on their sensitivity to subtle content differences and resistance tostyle biases. Extensive experiments demonstrate that RM-BENCH strongly corre-lates with policy model performance, making it a reliable reference for selectingreward models to align language models effectively. We evaluate nearly 40 re-ward models on RM-BENCH. Our results reveal that even state-of-the-art modelsachieve an average performance of only 46.6%, which falls short of random-levelaccuracy (50%) when faced with style bias interference. These findings highlightthe significant room for improvement in current reward models. Related code anddata are available at https://github.com/THU-KEG/RM-Bench."
    ]
  },
  {
    "url": "https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences",
    "paper_url": "https://openreview.net/pdf?id=QEHrmQPBdd",
    "paper_id": "QEHrmQPBdd",
    "contexts": [
      "stack exchange preference dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences",
      ".",
      "the Association for Computational Linguistics (ACL 2023), 2023.\nNathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4\nstack exchange preference dataset, 2023. URL https://huggingface.co/datasets/\nHuggingFaceH4/stack-exchange-preferences.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,"
    ]
  },
  {
    "url": "https://huggingface.co/Skywork",
    "paper_url": "https://openreview.net/pdf?id=QEHrmQPBdd",
    "paper_id": "QEHrmQPBdd",
    "contexts": [
      "arXiv:2305.20050, 2023.\nChris Yuhao Liu and Liang Zeng. Skywork reward model series. https://huggingface.co/\nSkywork, September 2024. URL https://huggingface.co/Skywork.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chat-\ngpt really correct? rigorous evaluation of large language models for code generation. Advances"
    ]
  },
  {
    "url": "https://huggingface.co/datasets/",
    "paper_url": "https://openreview.net/pdf?id=QEHrmQPBdd",
    "paper_id": "QEHrmQPBdd",
    "contexts": [
      "the Association for Computational Linguistics (ACL 2023), 2023.\nNathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4\nstack exchange preference dataset, 2023. URL https://huggingface.co/datasets/\nHuggingFaceH4/stack-exchange-preferences.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,"
    ]
  },
  {
    "url": "https://huggingface.co/Skywork.Jiawei",
    "paper_url": "https://openreview.net/pdf?id=QEHrmQPBdd",
    "paper_id": "QEHrmQPBdd",
    "contexts": [
      "arXiv:2305.20050, 2023.\nChris Yuhao Liu and Liang Zeng. Skywork reward model series. https://huggingface.co/\nSkywork, September 2024. URL https://huggingface.co/Skywork.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chat-\ngpt really correct? rigorous evaluation of large language models for code generation. Advances"
    ]
  },
  {
    "url": "https://aclanthology.org/L18-1012",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "dosi, and Evelina Fedorenko. The Natural Stories Corpus. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May\n2018. European Language Resources Association (ELRA). URL https://aclanthology.\norg/L18-1012.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A."
    ]
  },
  {
    "url": "https://linkinghub.elsevier.com/retrieve/pii/S089662732030605X",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "James J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Hu-man Intelligence. Neuron, 108(3):413–423, November 2020. ISSN 08966273. doi: 10.1016/j.neuron.2020.07.040. URL https://linkinghub.elsevier.com/retrieve/pii/S089662732030605X",
      "James J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Hu-\nman Intelligence. Neuron, 108(3):413–423, November 2020. ISSN 08966273. doi: 10.1016/\nj.neuron.2020.07.040. URL https://linkinghub.elsevier.com/retrieve/pii/\nS089662732030605X. Number: 3.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy"
    ]
  },
  {
    "url": "http://aclweb.org/anthology/W18-5446",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "ceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pp. 353–355, Brussels, Belgium, 2018. Association for Computational Linguis-\ntics. doi: 10.18653/v1/W18-5446. URL http://aclweb.org/anthology/W18-5446.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and\nSamuel R. Bowman. BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transac-"
    ]
  },
  {
    "url": "10.1016/",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "9405cc0d6169988371b2755e573cc28650d14dfe.\nMartin Schrimpf, Jonas Kubilius, Michael J. Lee, N. Apurva Ratan Murty, Robert Ajemian, and\nJames J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Hu-\nman Intelligence. Neuron, 108(3):413–423, November 2020. ISSN 08966273. doi: 10.1016/\nj.neuron.2020.07.040. URL https://linkinghub.elsevier.com/retrieve/pii/"
    ]
  },
  {
    "url": "https://linkinghub.elsevier.com/retrieve/pii/",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "James J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Hu-\nman Intelligence. Neuron, 108(3):413–423, November 2020. ISSN 08966273. doi: 10.1016/\nj.neuron.2020.07.040. URL https://linkinghub.elsevier.com/retrieve/pii/\nS089662732030605X. Number: 3.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy"
    ]
  },
  {
    "url": "http://aclweb.org/anthology/W18-5446.Alex",
    "paper_url": "https://openreview.net/pdf?id=aWXnKanInf",
    "paper_id": "aWXnKanInf",
    "contexts": [
      "ceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pp. 353–355, Brussels, Belgium, 2018. Association for Computational Linguis-\ntics. doi: 10.18653/v1/W18-5446. URL http://aclweb.org/anthology/W18-5446.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and\nSamuel R. Bowman. BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transac-"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2409.07703",
    "paper_url": "https://openreview.net/pdf?id=XmProj9cPs",
    "paper_id": "XmProj9cPs",
    "contexts": [
      "Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming\nZhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data\nscience experts?, 2024. URL https://arxiv.org/abs/2409.07703.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau\nYih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data"
    ]
  },
  {
    "url": "https://doi.org/10.48550/arXiv.2306.14898",
    "paper_url": "https://openreview.net/pdf?id=XmProj9cPs",
    "paper_id": "XmProj9cPs",
    "contexts": [
      "and benchmarking interactive coding with execution feedback. CoRR, abs/2306.14898, 2023. doi:10.48550/arXiv.2306.14898. URL https://doi.org/10.48550/arXiv.2306.14898.",
      "John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing\nand benchmarking interactive coding with execution feedback. CoRR, abs/2306.14898, 2023. doi:\n10.48550/arXiv.2306.14898. URL https://doi.org/10.48550/arXiv.2306.14898.\nJohn Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan,\nand Ofir Press. Swe-agent: Agent computer interfaces enable software engineering language"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2409.07703.Yuhang",
    "paper_url": "https://openreview.net/pdf?id=XmProj9cPs",
    "paper_id": "XmProj9cPs",
    "contexts": [
      "Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming\nZhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data\nscience experts?, 2024. URL https://arxiv.org/abs/2409.07703.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau\nYih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data"
    ]
  },
  {
    "url": "https://doi.org/10.48550/arXiv.2306.14898.John",
    "paper_url": "https://openreview.net/pdf?id=XmProj9cPs",
    "paper_id": "XmProj9cPs",
    "contexts": [
      "John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing\nand benchmarking interactive coding with execution feedback. CoRR, abs/2306.14898, 2023. doi:\n10.48550/arXiv.2306.14898. URL https://doi.org/10.48550/arXiv.2306.14898.\nJohn Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan,\nand Ofir Press. Swe-agent: Agent computer interfaces enable software engineering language"
    ]
  },
  {
    "url": "https://github.com/allenai/OLMo/tree/main",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "5We used OLMo and Dolma from the official repository"
    ]
  },
  {
    "url": "https://huggingface.co/datasets/ncbi/pubmed",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "7We randomly sample 205k instances for each dataset.8Datasets in huggingface9We slightly modified the dataset to our setting of which details are in Appendix B.1"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:227231454",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "ral language processing: A survey. ArXiv, abs/2012.09823, 2020. URL https://api.semanticscholar.org/CorpusID:227231454",
      ".",
      "Learning, pp. 2397–2430. PMLR, 2023.\nMagdalena Biesialska, Katarzyna Biesialska, and Marta Ruiz Costa-jussà. Continual lifelong learning in natu-\nral language processing: A survey. ArXiv, abs/2012.09823, 2020. URL https://api.semanticscholar.\norg/CorpusID:227231454.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:251223709",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "bank for pretrained transformers. In Natural Language Processing and Chinese Computing, 2022b. URLhttps://api.semanticscholar.org/CorpusID:251223709",
      ".",
      "Damai Dai, Wen-Jie Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. Neural knowledge\nbank for pretrained transformers. In Natural Language Processing and Chinese Computing, 2022b. URL\nhttps://api.semanticscholar.org/CorpusID:251223709.\nShibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Rupam Mahmood,\nand Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 – 774, 2024. URL"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:259251905",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "and Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 – 774, 2024. URLhttps://api.semanticscholar.org/CorpusID:259251905",
      ".",
      "Shibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Rupam Mahmood,\nand Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 – 774, 2024. URL\nhttps://api.semanticscholar.org/CorpusID:259251905.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating factual knowledge\nin pretrained language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings"
    ]
  },
  {
    "url": "https://openreview.net/forum?id=vfsRB5MImo9",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. Towards continual knowledge learning of language models. In International Con-\nference on Learning Representations, 2022. URL https://openreview.net/forum?id=vfsRB5MImo9.\nYujin Kim, Jaehong Yoon, Seonghyeon Ye, Sung Ju Hwang, and Se-young Yun. Carpe diem: on the evaluation\nof world knowledge in lifelong language models. NACCL 2024, 2023."
    ]
  },
  {
    "url": "https://openreview.net/forum?id=-h6WAS6eE4",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=-h6WAS6eE4.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:202539551",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      ", 2019. URL https://api.semanticscholar.org/CorpusID:202539551.",
      "Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. Language models as knowledge bases? In Conference on Empirical Methods in Natural\nLanguage Processing, 2019. URL https://api.semanticscholar.org/CorpusID:202539551.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer."
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:270068372",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "circuits in pretrained transformers. ArXiv, abs/2405.17969, 2024. URL https://api.semanticscholar.org/CorpusID:270068372",
      ".",
      "arXiv:2407.21092, 2024.\nYunzhi Yao, Ningyu Zhang, Zekun Xi, Meng Wang, Ziwen Xu, Shumin Deng, and Huajun Chen. Knowledge\ncircuits in pretrained transformers. ArXiv, abs/2405.17969, 2024. URL https://api.semanticscholar.\norg/CorpusID:270068372.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:251223709.Shibhansh",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Damai Dai, Wen-Jie Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. Neural knowledge\nbank for pretrained transformers. In Natural Language Processing and Chinese Computing, 2022b. URL\nhttps://api.semanticscholar.org/CorpusID:251223709.\nShibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Rupam Mahmood,\nand Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 – 774, 2024. URL"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:259251905.Qingxiu",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Shibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Rupam Mahmood,\nand Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632:768 – 774, 2024. URL\nhttps://api.semanticscholar.org/CorpusID:259251905.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating factual knowledge\nin pretrained language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings"
    ]
  },
  {
    "url": "https://openreview.net/forum?id=vfsRB5MImo9.Yujin",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. Towards continual knowledge learning of language models. In International Con-\nference on Learning Representations, 2022. URL https://openreview.net/forum?id=vfsRB5MImo9.\nYujin Kim, Jaehong Yoon, Seonghyeon Ye, Sung Ju Hwang, and Se-young Yun. Carpe diem: on the evaluation\nof world knowledge in lifelong language models. NACCL 2024, 2023."
    ]
  },
  {
    "url": "https://openreview.net/forum?id=-h6WAS6eE4.Todor",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=-h6WAS6eE4.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and"
    ]
  },
  {
    "url": "https://api.semanticscholar.org/CorpusID:202539551.Colin",
    "paper_url": "https://openreview.net/pdf?id=eHehzSDUFp",
    "paper_id": "eHehzSDUFp",
    "contexts": [
      "Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. Language models as knowledge bases? In Conference on Empirical Methods in Natural\nLanguage Processing, 2019. URL https://api.semanticscholar.org/CorpusID:202539551.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer."
    ]
  }
]